{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "import pygame\n",
    "\n",
    "class DepthSensor:\n",
    "    def __init__(self):\n",
    "        # Load the pre-trained Haar Cascade Classifiers for face detection, eye detection, and smile detection\n",
    "        self.face_cascade = cv2.CascadeClassifier('C:\\\\Users\\\\baaqi\\\\Desktop\\\\Hackathon-Project-2024-UVM\\\\haarcascade_frontalface_default.xml')\n",
    "        self.eye_cascade = cv2.CascadeClassifier('C:\\\\Users\\\\baaqi\\\\Desktop\\\\Hackathon-Project-2024-UVM\\\\haarcascade_eye.xml')\n",
    "        self.smile_cascade = cv2.CascadeClassifier('C:\\\\Users\\\\baaqi\\\\Desktop\\\\Hackathon-Project-2024-UVM\\\\haarcascade_smile.xml')\n",
    "\n",
    "    def capture_facial_data(self):\n",
    "        print(\"Capturing facial data using depth sensor (camera)...\")\n",
    "        \n",
    "        video_capture = cv2.VideoCapture(0)\n",
    "        start_time = time.time()  # Record the start time\n",
    "\n",
    "        while True:\n",
    "            ret, frame = video_capture.read()\n",
    "            if not ret:\n",
    "                print(\"Error: Could not read frame.\")\n",
    "                break\n",
    "\n",
    "            # Flip the frame horizontally\n",
    "            frame = cv2.flip(frame, 1)\n",
    "\n",
    "            gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "            # Detect faces in the frame\n",
    "            faces = self.face_cascade.detectMultiScale(gray_frame, scaleFactor=1.1, minNeighbors=5)\n",
    "\n",
    "            for (x, y, w, h) in faces:\n",
    "                cv2.rectangle(frame, (x, y), (x + w, y + h), (255, 0, 0), 2)\n",
    "                \n",
    "                # Crop the face for emotion recognition\n",
    "                face = gray_frame[y:y + h, x:x + w]\n",
    "                \n",
    "                # Detect eyes within the detected face region\n",
    "                eyes = self.eye_cascade.detectMultiScale(face)\n",
    "                if len(eyes) > 0:  # If eyes are detected\n",
    "                    for (ex, ey, ew, eh) in eyes:\n",
    "                        cv2.rectangle(frame, (x + ex, y + ey), (x + ex + ew, y + ey + eh), (0, 255, 0), 2)\n",
    "\n",
    "                # Detect smiles within the detected face region\n",
    "                smiles = self.smile_cascade.detectMultiScale(face, scaleFactor=1.8, minNeighbors=20)\n",
    "                if len(smiles) > 0:  # If smiles are detected\n",
    "                    for (sx, sy, sw, sh) in smiles:\n",
    "                        cv2.rectangle(frame, (x + sx, y + sy), (x + sx + sw, y + sy + sh), (255, 255, 0), 2)\n",
    "\n",
    "                # Analyze facial emotion\n",
    "                facial_emotion = self.analyze_facial_expressions(face)\n",
    "                cv2.putText(frame, facial_emotion, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255, 0, 0), 2)\n",
    "\n",
    "            cv2.imshow('Video', frame)\n",
    "\n",
    "            # Check for 5 seconds limit\n",
    "            elapsed_time = time.time() - start_time\n",
    "            if elapsed_time > 5:  # Stop after 5 seconds\n",
    "                print(\"Time limit reached. Exiting facial data capture.\")\n",
    "                break\n",
    "\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):  # Allow user to exit early\n",
    "                break\n",
    "\n",
    "        # Release the video capture and destroy all OpenCV windows\n",
    "        video_capture.release()\n",
    "        cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_facial_expressions(self, face):\n",
    "        # Directly process the face for emotion recognition\n",
    "        face = cv2.resize(face, (48, 48))  # Resize the face to a 48x48 size\n",
    "        face = face.astype('float32') / 255.0  # Normalize the face pixel values to range [0, 1]\n",
    "        face = np.expand_dims(face, axis=0)  # Add a batch dimension for model input\n",
    "\n",
    "        # Example prediction: Replace this with your actual model prediction logic\n",
    "        emotion_label = np.random.randint(0, 8)  # Adjusted to 8 for Drowsy\n",
    "\n",
    "         # The emotion dictionary\n",
    "        emotion_dict = {\n",
    "            0: \"Angry\",\n",
    "            1: \"Disgust\",\n",
    "            2: \"Fear\",\n",
    "            3: \"Happy\",\n",
    "            4: \"Sad\",\n",
    "            5: \"Surprise\",\n",
    "            6: \"Neutral\",\n",
    "            7: \"Drowsy\"  # Added Drowsy\n",
    "        }\n",
    "        return emotion_dict[emotion_label]  # Return the detected emotion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class for infrared sensor\n",
    "class InfraredSensor:\n",
    "    def monitor_temperature(self):\n",
    "        print(\"Monitoring temperature using infrared sensor...\")\n",
    "\n",
    "    def detect_stress(self):\n",
    "        return \"Stress\"  \n",
    "\n",
    "# Class for voice analysis\n",
    "class VoiceAnalysis: \n",
    "    def analyze_voice(self):\n",
    "        print(\"Analyzing voice tone and pitch...\")\n",
    "\n",
    "    def get_emotional_state_from_voice(self):\n",
    "        # Replace with actual analysis logic\n",
    "        return \"Neutral\"  \n",
    "\n",
    "# Class for gesture recognition\n",
    "class GestureRecognition:\n",
    "    def recognize_gestures(self):\n",
    "        print(\"Recognizing gestures...\")\n",
    "\n",
    "    def get_gesture_emotional_state(self):\n",
    "        # Replace with actual gesture recognition logic\n",
    "        return \"Neutral\"  \n",
    "\n",
    "# Class for behavioral monitoring\n",
    "class BehavioralMonitoring:\n",
    "    def monitor_driving_behavior(self):\n",
    "        print(\"Monitoring driving behavior...\")\n",
    "\n",
    "    def assess_driving_behavior(self):\n",
    "        # Replace with actual driving behavior assessment logic\n",
    "        return \"Normal\"  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Capturing facial data using depth sensor (camera)...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'DepthSensor' object has no attribute 'analyze_facial_expressions'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 75\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     74\u001b[0m     emotion_recognition_system \u001b[38;5;241m=\u001b[39m EmotionRecognitionSystem()\n\u001b[1;32m---> 75\u001b[0m     \u001b[43memotion_recognition_system\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecognize_emotions\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[9], line 23\u001b[0m, in \u001b[0;36mEmotionRecognitionSystem.recognize_emotions\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrecognize_emotions\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m---> 23\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdepth_sensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcapture_facial_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfrared_sensor\u001b[38;5;241m.\u001b[39mmonitor_temperature()\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvoice_analysis\u001b[38;5;241m.\u001b[39manalyze_voice()\n",
      "Cell \u001b[1;32mIn[6], line 52\u001b[0m, in \u001b[0;36mDepthSensor.capture_facial_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     49\u001b[0m             cv2\u001b[38;5;241m.\u001b[39mrectangle(frame, (x \u001b[38;5;241m+\u001b[39m sx, y \u001b[38;5;241m+\u001b[39m sy), (x \u001b[38;5;241m+\u001b[39m sx \u001b[38;5;241m+\u001b[39m sw, y \u001b[38;5;241m+\u001b[39m sy \u001b[38;5;241m+\u001b[39m sh), (\u001b[38;5;241m255\u001b[39m, \u001b[38;5;241m255\u001b[39m, \u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;66;03m# Analyze facial emotion\u001b[39;00m\n\u001b[1;32m---> 52\u001b[0m     facial_emotion \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43manalyze_facial_expressions\u001b[49m(face)\n\u001b[0;32m     53\u001b[0m     cv2\u001b[38;5;241m.\u001b[39mputText(frame, facial_emotion, (x, y \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m10\u001b[39m), cv2\u001b[38;5;241m.\u001b[39mFONT_HERSHEY_SIMPLEX, \u001b[38;5;241m0.9\u001b[39m, (\u001b[38;5;241m255\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m     55\u001b[0m cv2\u001b[38;5;241m.\u001b[39mimshow(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVideo\u001b[39m\u001b[38;5;124m'\u001b[39m, frame)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DepthSensor' object has no attribute 'analyze_facial_expressions'"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Playlist dictionary\n",
    "playlists = {\n",
    "    \"Angry\": (\"C:\\\\Users\\\\baaqi\\\\Downloads\\\\Faint (Official Music Video) [4K UPGRADE]  Linkin Park.mp3\", \"https://www.youtube.com/watch?v=LYU-8IFcDPw&ab_channel=LinkinPark\"),\n",
    "    \"Disgust\": (\"C:\\\\Users\\\\baaqi\\\\Downloads\\\\Cee Lo Green  Forget You (Extended Clean Radio Edit).mp3\", \"https://www.youtube.com/watch?v=UZnaQUIZXmk&ab_channel=BLOODMOVECLEANVERSIONS2\"),\n",
    "    \"Fear\": (\"C:\\\\Users\\\\baaqi\\\\Downloads\\\\Radiohead - Creep (Lyrics) (From Fear Street Part 1_ 1994).mp3\", \"https://www.youtube.com/watch?v=-dR977j38BI&ab_channel=Newfunvibe\"),\n",
    "    \"Happy\": (\"C:\\\\Users\\\\baaqi\\\\Downloads\\\\Pharrell Williams - Happy (Lyrics).mp3.crdownload\", \"https://www.youtube.com/watch?v=jv-pYB0Qw9A&ab_channel=AnimeOracle\"),\n",
    "    \"Sad\": (\"C:\\\\Users\\\\baaqi\\\\Downloads\\\\Someone Like You - Adele (Lyrics).mp3.crdownload\", \"https://www.youtube.com/watch?v=z7GCiVTlv04&ab_channel=Pillow\"),\n",
    "    \"Surprise\": (\"C:\\\\Users\\\\baaqi\\\\Downloads\\\\Avicii - Wake Me Up (Official Lyric Video).mp3.crdownload\", \"https://www.youtube.com/watch?v=5y_KJAg8bHI&ab_channel=AviciiOfficialVEVO\"),\n",
    "    \"Neutral\": (\"C:\\\\Users\\\\baaqi\\\\Downloads\\\\Coldplay - Viva La Vida (Official Video).mp3\", \"https://www.youtube.com/watch?v=dvgZkm1xWPE\"),\n",
    "    \"Drowsy\": (\"C:\\\\Users\\\\baaqi\\\\Downloads\\\\Maroon 5 - Sugar (Lyrics).mp3\", \"https://www.youtube.com/watch?v=N1BcpzPGlYQ&ab_channel=7clouds\"),\n",
    "    \"Stress\": (\"C:\\\\Users\\\\baaqi\\\\Downloads\\\\@laufey - From The Start (Lyrics).mp3.crdownload\", \"https://www.youtube.com/watch?v=rHvQakk1zMA&ab_channel=DanMusic\"),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Capturing facial data using depth sensor (camera)...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'DepthSensor' object has no attribute 'analyze_facial_expressions'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 63\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     62\u001b[0m     emotion_recognition_system \u001b[38;5;241m=\u001b[39m EmotionRecognitionSystem()\n\u001b[1;32m---> 63\u001b[0m     \u001b[43memotion_recognition_system\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecognize_emotions\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[5], line 11\u001b[0m, in \u001b[0;36mEmotionRecognitionSystem.recognize_emotions\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrecognize_emotions\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m---> 11\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdepth_sensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcapture_facial_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfrared_sensor\u001b[38;5;241m.\u001b[39mmonitor_temperature()\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvoice_analysis\u001b[38;5;241m.\u001b[39manalyze_voice()\n",
      "Cell \u001b[1;32mIn[1], line 52\u001b[0m, in \u001b[0;36mDepthSensor.capture_facial_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     49\u001b[0m             cv2\u001b[38;5;241m.\u001b[39mrectangle(frame, (x \u001b[38;5;241m+\u001b[39m sx, y \u001b[38;5;241m+\u001b[39m sy), (x \u001b[38;5;241m+\u001b[39m sx \u001b[38;5;241m+\u001b[39m sw, y \u001b[38;5;241m+\u001b[39m sy \u001b[38;5;241m+\u001b[39m sh), (\u001b[38;5;241m255\u001b[39m, \u001b[38;5;241m255\u001b[39m, \u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;66;03m# Analyze facial emotion\u001b[39;00m\n\u001b[1;32m---> 52\u001b[0m     facial_emotion \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43manalyze_facial_expressions\u001b[49m(face)\n\u001b[0;32m     53\u001b[0m     cv2\u001b[38;5;241m.\u001b[39mputText(frame, facial_emotion, (x, y \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m10\u001b[39m), cv2\u001b[38;5;241m.\u001b[39mFONT_HERSHEY_SIMPLEX, \u001b[38;5;241m0.9\u001b[39m, (\u001b[38;5;241m255\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m     55\u001b[0m cv2\u001b[38;5;241m.\u001b[39mimshow(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVideo\u001b[39m\u001b[38;5;124m'\u001b[39m, frame)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DepthSensor' object has no attribute 'analyze_facial_expressions'"
     ]
    }
   ],
   "source": [
    "class EmotionRecognitionSystem:\n",
    "    def __init__(self):\n",
    "        self.depth_sensor = DepthSensor()\n",
    "        self.infrared_sensor = InfraredSensor()\n",
    "        self.voice_analysis = VoiceAnalysis()\n",
    "        self.gesture_recognition = GestureRecognition()\n",
    "        self.behavioral_monitoring = BehavioralMonitoring()\n",
    "        pygame.init()  # Initialize Pygame for audio playback\n",
    "\n",
    "    def recognize_emotions(self):\n",
    "        self.depth_sensor.capture_facial_data()\n",
    "        self.infrared_sensor.monitor_temperature()\n",
    "        self.voice_analysis.analyze_voice()\n",
    "        self.gesture_recognition.recognize_gestures()\n",
    "        self.behavioral_monitoring.monitor_driving_behavior()\n",
    "\n",
    "        facial_emotion = self.depth_sensor.analyze_facial_expressions(np.zeros((48, 48)))  # Placeholder for the face\n",
    "        stress_emotion = self.infrared_sensor.detect_stress()\n",
    "        voice_emotion = self.voice_analysis.get_emotional_state_from_voice()\n",
    "        gesture_emotion = self.gesture_recognition.get_gesture_emotional_state()\n",
    "        driving_behavior = self.behavioral_monitoring.assess_driving_behavior()\n",
    "\n",
    "        # Print detected emotions\n",
    "        print(\"Facial Emotion:\", facial_emotion)\n",
    "        print(\"Stress Emotion:\", stress_emotion)\n",
    "        print(\"Voice Emotion:\", voice_emotion)\n",
    "        print(\"Gesture Emotion:\", gesture_emotion)\n",
    "        print(\"Driving Behavior:\", driving_behavior)\n",
    "\n",
    "        # Combine emotions to determine final mood\n",
    "        final_mood = facial_emotion  # You can customize this logic as needed\n",
    "        \n",
    "        # Print the detected mood before playing the song\n",
    "        print(f\"Detected mood: {final_mood}\")\n",
    "\n",
    "        # Play the corresponding song\n",
    "        self.play_song(final_mood)\n",
    "\n",
    "        # Trigger adaptive responses based on detected emotions\n",
    "        self.trigger_responses(facial_emotion, stress_emotion, voice_emotion, gesture_emotion, driving_behavior)\n",
    "        def play_song(self, mood):\n",
    "            \n",
    "            if mood in playlists:\n",
    "                song_path, _ = playlists[mood]\n",
    "            print(f\"Playing song for mood: {mood}\")\n",
    "            # Load and play the music\n",
    "            pygame.mixer.music.load(song_path)\n",
    "            pygame.mixer.music.play()\n",
    "\n",
    "            # Keep the music playing until it stops\n",
    "            while pygame.mixer.music.get_busy():\n",
    "                pygame.time.Clock().tick(10)  # Check if music is still playing\n",
    "            else:\n",
    "                print(f\"No song found for mood: {mood}\")\n",
    "\n",
    "def trigger_responses(self, facial_emotion, stress_emotion, voice_emotion, gesture_emotion, driving_behavior):\n",
    "        # Implement adaptive responses based on detected emotions\n",
    "        print(\"Triggering adaptive responses based on detected emotions...\")\n",
    "        # Custom logic can be added here based on the various emotional states detected\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    emotion_recognition_system = EmotionRecognitionSystem()\n",
    "    emotion_recognition_system.recognize_emotions()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
