{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "class DepthSensor:\n",
    "    def __init__(self):\n",
    "        # Load the pre-trained Haar Cascade Classifiers for face detection, eye detection, and smile detection\n",
    "        self.face_cascade = cv2.CascadeClassifier('C:\\\\Users\\\\baaqi\\\\Desktop\\\\Hackathon-Project-2024-UVM\\\\haarcascade_frontalface_default.xml')\n",
    "        self.eye_cascade = cv2.CascadeClassifier('C:\\\\Users\\\\baaqi\\\\Desktop\\\\Hackathon-Project-2024-UVM\\\\haarcascade_eye.xml')\n",
    "        self.smile_cascade = cv2.CascadeClassifier('C:\\\\Users\\\\baaqi\\\\Desktop\\\\Hackathon-Project-2024-UVM\\\\haarcascade_smile.xml')\n",
    "\n",
    "    def capture_facial_data(self):\n",
    "        print(\"Capturing facial data using depth sensor (camera)...\")\n",
    "        \n",
    "        video_capture = cv2.VideoCapture(0)\n",
    "        start_time = time.time()  # Record the start time\n",
    "\n",
    "        while True:\n",
    "            ret, frame = video_capture.read()\n",
    "            if not ret:\n",
    "                print(\"Error: Could not read frame.\")\n",
    "                break\n",
    "\n",
    "            gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "            # Detect faces in the frame\n",
    "            faces = self.face_cascade.detectMultiScale(gray_frame, scaleFactor=1.1, minNeighbors=5)\n",
    "\n",
    "            for (x, y, w, h) in faces:\n",
    "                cv2.rectangle(frame, (x, y), (x + w, y + h), (255, 0, 0), 2)\n",
    "                \n",
    "                # Crop the face for emotion recognition\n",
    "                face = gray_frame[y:y + h, x:x + w]\n",
    "                \n",
    "                # Detect eyes within the detected face region\n",
    "                eyes = self.eye_cascade.detectMultiScale(face)\n",
    "                if len(eyes) > 0:  # If eyes are detected\n",
    "                    for (ex, ey, ew, eh) in eyes:\n",
    "                        cv2.rectangle(frame, (x + ex, y + ey), (x + ex + ew, y + ey + eh), (0, 255, 0), 2)\n",
    "\n",
    "                # Detect smiles within the detected face region\n",
    "                smiles = self.smile_cascade.detectMultiScale(face, scaleFactor=1.8, minNeighbors=20)\n",
    "                if len(smiles) > 0:  # If smiles are detected\n",
    "                    for (sx, sy, sw, sh) in smiles:\n",
    "                        cv2.rectangle(frame, (x + sx, y + sy), (x + sx + sw, y + sy + sh), (255, 255, 0), 2)\n",
    "\n",
    "                # Analyze facial emotion\n",
    "                facial_emotion = self.analyze_facial_expressions(face)\n",
    "                cv2.putText(frame, facial_emotion, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255, 0, 0), 2)\n",
    "\n",
    "            cv2.imshow('Video', frame)\n",
    "\n",
    "            # Check for 5 seconds limit\n",
    "            elapsed_time = time.time() - start_time\n",
    "            if elapsed_time > 5:  # Stop after 5 seconds\n",
    "                print(\"Time limit reached. Exiting facial data capture.\")\n",
    "                break\n",
    "\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):  # Allow user to exit early\n",
    "                break\n",
    "\n",
    "        # Release the video capture and destroy all OpenCV windows\n",
    "        video_capture.release()\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "    def analyze_facial_expressions(self, face):\n",
    "        # Directly process the face for emotion recognition\n",
    "        face = cv2.resize(face, (48, 48))  # Resize the face to a 48x48 size\n",
    "        face = face.astype('float32') / 255.0  # Normalize the face pixel values to range [0, 1]\n",
    "        face = np.expand_dims(face, axis=0)  # Add a batch dimension for model input\n",
    "\n",
    "        # Example prediction: Replace this with your actual model prediction logic\n",
    "        emotion_label = np.random.randint(0, 7)  # Randomly pick an emotion for demonstration\n",
    "        \n",
    "        # The emotion dictionary\n",
    "        emotion_dict = {0: \"Angry\", 1: \"Disgust\", 2: \"Fear\", 3: \"Happy\", 4: \"Sad\", 5: \"Surprise\", 6: \"Neutral\"}\n",
    "        \n",
    "        return emotion_dict[emotion_label]  # Return the detected emotion\n",
    "\n",
    "\n",
    "# Class for infrared sensor\n",
    "class InfraredSensor:\n",
    "    def monitor_temperature(self):\n",
    "        print(\"Monitoring temperature using infrared sensor...\")\n",
    "\n",
    "    def detect_stress(self):\n",
    "        return \"Stress\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class for voice analysis\n",
    "class VoiceAnalysis: \n",
    "    def analyze_voice(self):\n",
    "        print(\"Analyzing voice tone and pitch...\")\n",
    "\n",
    "    def get_emotional_state_from_voice(self):\n",
    "        # Replace with actual analysis logic\n",
    "        return \"Neutral\"  \n",
    "\n",
    "\n",
    "# Class for gesture recognition\n",
    "class GestureRecognition:\n",
    "    def recognize_gestures(self):\n",
    "        print(\"Recognizing gestures...\")\n",
    "\n",
    "    def get_gesture_emotional_state(self):\n",
    "        # Replace with actual gesture recognition logic\n",
    "        return \"Neutral\"  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class for behavioral monitoring\n",
    "class BehavioralMonitoring:\n",
    "    def monitor_driving_behavior(self):\n",
    "        print(\"Monitoring driving behavior...\")\n",
    "\n",
    "    def assess_driving_behavior(self):\n",
    "        # Replace with actual driving behavior assessment logic\n",
    "        return \"Normal\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Capturing facial data using depth sensor (camera)...\n",
      "Time limit reached. Exiting facial data capture.\n",
      "Monitoring temperature using infrared sensor...\n",
      "Analyzing voice tone and pitch...\n",
      "Recognizing gestures...\n",
      "Monitoring driving behavior...\n",
      "Facial Emotion: Surprise\n",
      "Stress Emotion: Stress\n",
      "Voice Emotion: Neutral\n",
      "Gesture Emotion: Neutral\n",
      "Driving Behavior: Normal\n",
      "Triggering Relaxation Mode: Calming music and soft lighting.\n"
     ]
    }
   ],
   "source": [
    "class EmotionRecognitionSystem:\n",
    "    def __init__(self):\n",
    "        self.depth_sensor = DepthSensor()\n",
    "        self.infrared_sensor = InfraredSensor()\n",
    "        self.voice_analysis = VoiceAnalysis()\n",
    "        self.gesture_recognition = GestureRecognition()\n",
    "        self.behavioral_monitoring = BehavioralMonitoring()\n",
    "\n",
    "    def recognize_emotions(self):\n",
    "        self.depth_sensor.capture_facial_data()\n",
    "        self.infrared_sensor.monitor_temperature()\n",
    "        self.voice_analysis.analyze_voice()\n",
    "        self.gesture_recognition.recognize_gestures()\n",
    "        self.behavioral_monitoring.monitor_driving_behavior()\n",
    "\n",
    "        facial_emotion = self.depth_sensor.analyze_facial_expressions(np.zeros((48, 48)))  # Placeholder for the face\n",
    "        stress_emotion = self.infrared_sensor.detect_stress()\n",
    "        voice_emotion = self.voice_analysis.get_emotional_state_from_voice()\n",
    "        gesture_emotion = self.gesture_recognition.get_gesture_emotional_state()\n",
    "        driving_behavior = self.behavioral_monitoring.assess_driving_behavior()\n",
    "\n",
    "        print(\"Facial Emotion:\", facial_emotion)\n",
    "        print(\"Stress Emotion:\", stress_emotion)\n",
    "        print(\"Voice Emotion:\", voice_emotion)\n",
    "        print(\"Gesture Emotion:\", gesture_emotion)\n",
    "        print(\"Driving Behavior:\", driving_behavior)\n",
    "\n",
    "        # Trigger adaptive responses based on detected emotions\n",
    "        self.trigger_responses(facial_emotion, stress_emotion, voice_emotion, gesture_emotion, driving_behavior)\n",
    "\n",
    "    def trigger_responses(self, facial_emotion, stress_emotion, voice_emotion, gesture_emotion, driving_behavior):\n",
    "        if stress_emotion == \"Stress\" or voice_emotion == \"Frustration\":\n",
    "            print(\"Triggering Relaxation Mode: Calming music and soft lighting.\")\n",
    "        elif gesture_emotion == \"Agitation\" or driving_behavior == \"Erratic\":\n",
    "            print(\"Triggering Alertness Mode: Increase brightness and play energizing music.\")\n",
    "        else:\n",
    "            print(\"Normal driving conditions. No action required.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    system = EmotionRecognitionSystem()\n",
    "    system.recognize_emotions()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
